{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "vSMat73wNrXB",
    "ExecuteTime": {
     "end_time": "2024-05-04T11:54:44.340347Z",
     "start_time": "2024-05-04T11:54:37.271489Z"
    }
   },
   "source": [
    "%%capture\n",
    "!apt-get install libglu1-mesa\n",
    "!pip install sinabs tonic jupyter-notebook\n",
    "!pip install numpy torch"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "source": [
    "To remove later cell:\n",
    "\n",
    "- every neuron 16bit, every synapse 8bit. Follow the following table for layer:\n",
    "https://sinabs.readthedocs.io/en/v2.0.0/speck/overview.\n",
    "- The GUI samna was already installed using sinabs https://www.synsense.ai/products/samna/\n",
    "- (for macos detecting)$ system_profiler SPUSBDataType "
   ],
   "metadata": {
    "id": "rxCp8-7q2FO1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LOAD DATA"
   ],
   "metadata": {
    "id": "bBW1E22TvqA9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#### LOAD DATA ####\n",
    "\n",
    "import tonic\n",
    "from tonic.transforms import ToFrame\n",
    "from tonic.datasets import nmnist\n",
    "from tonic import transforms\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "root = \"/\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToFrame(sensor_size=tonic.datasets.DVSGesture.sensor_size, n_time_bins=16, include_incomplete=True),\n",
    "    lambda x: x.astype(np.float32),\n",
    "])\n",
    "\n",
    "testset = tonic.datasets.DVSGesture(save_to=\"data/\", train=False, transform=transform)\n",
    "trainset = tonic.datasets.DVSGesture(save_to=\"data/\", train=True, transform=transform)\n",
    "\n",
    "events, label = trainset[0]\n",
    "events[0].shape"
   ],
   "metadata": {
    "id": "VFsZwkliYdlP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "03a86054-3a60-458d-b6f6-178ffda28fb5",
    "ExecuteTime": {
     "end_time": "2024-05-03T19:46:41.446577Z",
     "start_time": "2024-05-03T19:46:41.306568Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 128, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DEFINE MODEL"
   ],
   "metadata": {
    "id": "3QpioG6ivsop"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#### DEFINE MODEL ###\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import List\n",
    "import sinabs\n",
    "import sinabs.layers as sl\n",
    "\n",
    "class DVSGestureNet(nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        conv = []\n",
    "        # dimensions at input of IF layer\n",
    "        # 64x64, 64x64, 32x32\n",
    "        channels = [2, 16, 64, 128, 64, 8]    #15 is the most we can do for the first conv layer\n",
    "        kernel_size = [2, 2, 2, 2, 2]\n",
    "        stride = [2, 2, 2, 2, 2]\n",
    "        \n",
    "        for i in range(5):\n",
    "            conv.append(nn.Conv2d(channels[i], channels[i+1], kernel_size=kernel_size[i], stride=stride[i]))\n",
    "            conv.append(nn.BatchNorm2d(channels[i+1]))\n",
    "            conv.append(sl.IAFSqueeze(*args, **kwargs))\n",
    "            #if i != 0:\n",
    "            #  conv.append(sl.SumPool2d(2, 2))\n",
    "\n",
    "\n",
    "        self.conv_fc = nn.Sequential(\n",
    "            *conv,\n",
    "\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(channels[-1] * 4 * 4, 512),\n",
    "            sl.IAFSqueeze(*args, **kwargs),\n",
    "\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 110),\n",
    "            sl.IAFSqueeze(*args, **kwargs),\n",
    "            nn.Linear(110,11),\n",
    "            #sl.SumPool2d((10,1), stride=(10,1)),\n",
    "            sl.IAFSqueeze(*args, **kwargs),\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.conv_fc(x)\n",
    "\n",
    "    def return_sequential(self):\n",
    "      return self.conv_fc"
   ],
   "metadata": {
    "id": "iMZO-xZzQCr3",
    "ExecuteTime": {
     "end_time": "2024-05-04T14:28:58.076213Z",
     "start_time": "2024-05-04T14:28:58.068434Z"
    }
   },
   "outputs": [],
   "execution_count": 127
  },
  {
   "cell_type": "code",
   "source": [
    "net = DVSGestureNet(batch_size=1)\n",
    "\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "\n",
    "# cpu_snn = snn_convert.to(device=\"cpu\")\n",
    "cpu_snn = net.return_sequential().to(device=\"cpu\")\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 128, 128), discretize=True, dvs_input=True)\n",
    "devkit_name = \"speck2edevkit\"\n",
    "\n",
    "# use the `to` method of DynapcnnNetwork to deploy the SNN to the devkit\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=\"auto\",monitor_layers=[-1, 'dvs'])\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")"
   ],
   "metadata": {
    "id": "Y-UHVcZcik8j",
    "ExecuteTime": {
     "end_time": "2024-05-04T14:28:59.711122Z",
     "start_time": "2024-05-04T14:28:58.338355Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n",
      "The SNN is deployed on the core: [0, 1, 3, 4, 2, 5, 6, 7]\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "cell_type": "code",
   "source": [
    "from torchsummary import summary\n",
    "net.return_sequential()\n",
    "#summary(net, (2,128,128))\n",
    "x = torch.randn(1,2,128,128)\n",
    "net(x)"
   ],
   "metadata": {
    "id": "af_rmepHirKn",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3c94bd38-b57e-4093-b37a-7f255610a4bb",
    "ExecuteTime": {
     "end_time": "2024-05-04T13:03:22.159946Z",
     "start_time": "2024-05-04T13:03:22.144762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0., -0., 0., -0., -0., -0., 0., 0., -0., -0., 0.]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [
    "# TRAIN MODEL"
   ],
   "metadata": {
    "id": "u7jNiSPSv3Sr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "epochs = 1\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "n_time_steps=16\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "shuffle = True\n",
    "\n",
    "snn_train_dataloader = DataLoader(trainset, batch_size=batch_size, drop_last=True, shuffle=True) #  num_workers=num_workers,\n",
    "snn_test_dataloader = DataLoader(testset, batch_size=batch_size, drop_last=True, shuffle=False) #  num_workers=num_workers,\n",
    "\n",
    "net = net.to(device=device)\n",
    "\n",
    "optimizer = Adam(params=net.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    # train_p_bar = tqdm(snn_train_dataloader)\n",
    "    for data, label in snn_train_dataloader:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 128, 128).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        loss = criterion(output, label)\n",
    "        #print(loss.device)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # detach the neuron states and activations from current computation graph(necessary)\n",
    "        for layer in net.modules():\n",
    "            if isinstance(layer, sl.StatefulLayer):\n",
    "                for name, buffer in layer.named_buffers():\n",
    "                    buffer.detach_()\n",
    "\n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - BPTT Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(snn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "            data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = net(data)\n",
    "            # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "            output = output.reshape(batch_size, n_time_steps, -1)\n",
    "            # accumulate all time-steps output for final prediction\n",
    "            output = output.sum(dim=1)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - BPTT Testing Model...\")\n",
    "\n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - BPTT accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ],
   "metadata": {
    "id": "B02foXVWOFfe",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "outputId": "9fd5e63c-7507-40d0-95ca-0b1de3149be0",
    "ExecuteTime": {
     "end_time": "2024-05-04T12:58:19.998302Z",
     "start_time": "2024-05-04T12:58:10.214139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 - BPTT Training Loss: 2.5083:   0%|          | 0/269 [17:00:52<?, ?it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[45], line 40\u001B[0m\n\u001B[1;32m     37\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(output, label)\n\u001B[1;32m     38\u001B[0m \u001B[38;5;66;03m#print(loss.device)\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;66;03m# backward\u001B[39;00m\n\u001B[0;32m---> 40\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     41\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     43\u001B[0m \u001B[38;5;66;03m# detach the neuron states and activations from current computation graph(necessary)\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/torch/_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[1;32m    521\u001B[0m     )\n\u001B[0;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/torch/autograd/__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/torch/autograd/function.py:277\u001B[0m, in \u001B[0;36mBackwardCFunction.apply\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m    276\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mBackwardCFunction\u001B[39;00m(_C\u001B[38;5;241m.\u001B[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001B[0;32m--> 277\u001B[0m     \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs):\n\u001B[1;32m    278\u001B[0m         \u001B[38;5;66;03m# _forward_cls is defined by derived class\u001B[39;00m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;66;03m# The user should define either backward or vjp but never both.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m         backward_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mbackward  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    281\u001B[0m         vjp_fn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cls\u001B[38;5;241m.\u001B[39mvjp  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DEPLOY MODEL ON SPECK"
   ],
   "metadata": {
    "id": "C21nrRo4vv1F"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sinabs.backend.dynapcnn import io\n",
    "print(io.device_types)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vrkJUINhYRvd",
    "outputId": "29713401-7264-4dad-db8a-d686e3f1cde3",
    "ExecuteTime": {
     "end_time": "2024-05-04T13:02:05.281956Z",
     "start_time": "2024-05-04T13:02:05.277844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'speck': 'speck', 'speck2b': 'Speck2bTestboard', 'speck2devkit': 'Speck2DevKit', 'speck2btiny': 'Speck2bDevKitTiny', 'speck2e': 'Speck2eTestBoard', 'speck2edevkit': 'Speck2eDevKit', 'speck2fmodule': 'Speck2fModuleDevKit', 'speck2fdevkit': 'Speck2fDevKit', 'dynapse1devkit': 'Dynapse1DevKit', 'davis346': 'Davis 346', 'davis240': 'Davis 240', 'dvxplorer': 'DVXplorer', 'pollendevkit': 'PollenDevKit', 'dynapcnndevkit': 'DynapcnnDevKit', 'dynapse2': 'DYNAP-SE2 DevBoard', 'dynapse2_stack': 'DYNAP-SE2 Stack'}\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "cell_type": "code",
   "source": [
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "\n",
    "# cpu_snn = snn_convert.to(device=\"cpu\")\n",
    "cpu_snn = net.return_sequential().to(device=\"cpu\")\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 128, 128), discretize=True, dvs_input=True)\n",
    "devkit_name = \"speck2edevkit\"\n",
    "\n",
    "# use the `to` method of DynapcnnNetwork to deploy the SNN to the devkit\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=\"auto\",monitor_layers=[-1, 'dvs'])\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")"
   ],
   "metadata": {
    "id": "R4i3dSHC83S2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "outputId": "2531c96e-2c95-439e-d2f7-fcd05a795798",
    "ExecuteTime": {
     "end_time": "2024-05-04T13:02:43.015513Z",
     "start_time": "2024-05-04T13:02:42.945339Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No valid mapping found",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[67], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m devkit_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspeck2edevkit\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# use the `to` method of DynapcnnNetwork to deploy the SNN to the devkit\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mdynapcnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevkit_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchip_layers_ordering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mauto\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mmonitor_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdvs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe SNN is deployed on the core: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdynapcnn\u001B[38;5;241m.\u001B[39mchip_layers_ordering\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/sinabs/backend/dynapcnn/dynapcnn_network.py:147\u001B[0m, in \u001B[0;36mDynapcnnNetwork.to\u001B[0;34m(self, device, chip_layers_ordering, monitor_layers, config_modifier, slow_clk_frequency)\u001B[0m\n\u001B[1;32m    144\u001B[0m device_name, _ \u001B[38;5;241m=\u001B[39m parse_device_id(device)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m device_name \u001B[38;5;129;01min\u001B[39;00m ChipFactory\u001B[38;5;241m.\u001B[39msupported_devices:  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;66;03m# Generate config\u001B[39;00m\n\u001B[0;32m--> 147\u001B[0m     config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchip_layers_ordering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchip_layers_ordering\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    149\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmonitor_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmonitor_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_modifier\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_modifier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;66;03m# Apply configuration to device\u001B[39;00m\n\u001B[1;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msamna_device \u001B[38;5;241m=\u001B[39m open_device(device)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/sinabs/backend/dynapcnn/dynapcnn_network.py:339\u001B[0m, in \u001B[0;36mDynapcnnNetwork.make_config\u001B[0;34m(self, chip_layers_ordering, device, monitor_layers, config_modifier)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmake_config\u001B[39m(\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    293\u001B[0m     chip_layers_ordering: Union[Sequence[\u001B[38;5;28mint\u001B[39m], \u001B[38;5;28mstr\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    296\u001B[0m     config_modifier\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    297\u001B[0m ):\n\u001B[1;32m    298\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Prepare and output the `samna` DYNAPCNN configuration for this network.\u001B[39;00m\n\u001B[1;32m    299\u001B[0m \n\u001B[1;32m    300\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;124;03m            If the generated configuration is not valid for the specified device.\u001B[39;00m\n\u001B[1;32m    338\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 339\u001B[0m     config, is_compatible \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_config\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchip_layers_ordering\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchip_layers_ordering\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    341\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    342\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmonitor_layers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmonitor_layers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    343\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig_modifier\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig_modifier\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    344\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    345\u001B[0m     \u001B[38;5;66;03m# Validate config\u001B[39;00m\n\u001B[1;32m    346\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_compatible:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/sinabs/backend/dynapcnn/dynapcnn_network.py:252\u001B[0m, in \u001B[0;36mDynapcnnNetwork._make_config\u001B[0;34m(self, chip_layers_ordering, device, monitor_layers, config_modifier)\u001B[0m\n\u001B[1;32m    250\u001B[0m \u001B[38;5;66;03m# Figure out layer ordering\u001B[39;00m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chip_layers_ordering \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 252\u001B[0m     chip_layers_ordering \u001B[38;5;241m=\u001B[39m \u001B[43mconfig_builder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_valid_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    254\u001B[0m     \u001B[38;5;66;03m# Truncate chip_layers_ordering just in case a longer list is passed\u001B[39;00m\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m has_dvs_layer:\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/sinabs/backend/dynapcnn/config_builder.py:78\u001B[0m, in \u001B[0;36mConfigBuilder.get_valid_mapping\u001B[0;34m(cls, model)\u001B[0m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_valid_mapping\u001B[39m(\u001B[38;5;28mcls\u001B[39m, model: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDynapcnnNetwork\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m     65\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Find a valid set of layers for a given model.\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \n\u001B[1;32m     67\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;124;03m    model is mapped is the value of the i-th entry in the list.\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 78\u001B[0m     mapping \u001B[38;5;241m=\u001B[39m \u001B[43mget_valid_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mcls\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_constraints\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     79\u001B[0m     \u001B[38;5;66;03m# turn the mapping into a dict\u001B[39;00m\n\u001B[1;32m     80\u001B[0m     mapping \u001B[38;5;241m=\u001B[39m {m[\u001B[38;5;241m0\u001B[39m]: m[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m mapping}\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/sinabs/backend/dynapcnn/mapping.py:73\u001B[0m, in \u001B[0;36mget_valid_mapping\u001B[0;34m(model, constraints)\u001B[0m\n\u001B[1;32m     70\u001B[0m \u001B[38;5;66;03m# Call mapping\u001B[39;00m\n\u001B[1;32m     71\u001B[0m new_graph \u001B[38;5;241m=\u001B[39m edmonds(graph, \u001B[38;5;241m0\u001B[39m, \u001B[38;5;28mlen\u001B[39m(graph) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 73\u001B[0m netmap \u001B[38;5;241m=\u001B[39m \u001B[43mrecover_mapping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_mapping\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m netmap\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/sinabs/backend/dynapcnn/mapping.py:191\u001B[0m, in \u001B[0;36mrecover_mapping\u001B[0;34m(graph, layer_mapping)\u001B[0m\n\u001B[1;32m    189\u001B[0m             mapping\u001B[38;5;241m.\u001B[39mappend((i, edge\u001B[38;5;241m.\u001B[39mt \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mlen\u001B[39m(layer_mapping) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m    190\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapping) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(layer_mapping):\n\u001B[0;32m--> 191\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo valid mapping found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    192\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mapping\n",
      "\u001B[0;31mValueError\u001B[0m: No valid mapping found"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-04T13:02:08.827972Z",
     "start_time": "2024-05-04T13:02:08.798265Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sinabs.backend.dynapcnn.dynapcnn_visualizer import DynapcnnVisualizer\n",
    "\n",
    "\n",
    "visualizer = DynapcnnVisualizer(\n",
    "    window_scale=(4, 8),\n",
    "    dvs_shape=(128, 128),\n",
    "    add_power_monitor_plot=True,\n",
    "    # add_readout_plot=True,\n",
    "    spike_collection_interval=500\n",
    ")\n",
    "\n",
    "visualizer.connect(dynapcnn)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-04 15:02:08.800] [Graph] [warning] Graph is destroyed without any start! Graph is required to start manually to work.\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Model has to be ported to chip.\nHint: Call `.to()` method on the model. ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mConnectionError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[64], line 12\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msinabs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbackend\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdynapcnn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdynapcnn_visualizer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DynapcnnVisualizer\n\u001B[1;32m      4\u001B[0m visualizer \u001B[38;5;241m=\u001B[39m DynapcnnVisualizer(\n\u001B[1;32m      5\u001B[0m     window_scale\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m8\u001B[39m),\n\u001B[1;32m      6\u001B[0m     dvs_shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m128\u001B[39m, \u001B[38;5;241m128\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     spike_collection_interval\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m500\u001B[39m\n\u001B[1;32m     10\u001B[0m )\n\u001B[0;32m---> 12\u001B[0m \u001B[43mvisualizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdynapcnn\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/Speck_DVS_Cappocaccia/lib/python3.8/site-packages/sinabs/backend/dynapcnn/dynapcnn_visualizer.py:393\u001B[0m, in \u001B[0;36mDynapcnnVisualizer.connect\u001B[0;34m(self, dynapcnn_network, disjoint_process)\u001B[0m\n\u001B[1;32m    391\u001B[0m \u001B[38;5;66;03m# Checks for the visualizer to work correctly.\u001B[39;00m\n\u001B[1;32m    392\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(dynapcnn_network, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msamna_device\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[0;32m--> 393\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mConnectionError\u001B[39;00m(\n\u001B[1;32m    394\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mModel has to be ported to chip.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    395\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHint: Call `.to()` method on the model. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    396\u001B[0m     )\n\u001B[1;32m    398\u001B[0m config \u001B[38;5;241m=\u001B[39m dynapcnn_network\u001B[38;5;241m.\u001B[39msamna_config\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m config\u001B[38;5;241m.\u001B[39mdvs_layer\u001B[38;5;241m.\u001B[39mmonitor_enable:\n",
      "\u001B[0;31mConnectionError\u001B[0m: Model has to be ported to chip.\nHint: Call `.to()` method on the model. "
     ]
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
